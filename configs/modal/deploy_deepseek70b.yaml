model_id: deepseek-ai/DeepSeek-R1-Distill-Llama-70B
model_rev: b1c0b44b4369b597ad119a196caf79a9c40e141e
target_input_concurrency: 8
max_input_concurrency: 16
vllm_cfg:
deployment_config:
  num_nodes: 1
  gpus_per_node: 1
  gpu_type: B200
timeout: 10800
scaledown_window: 900
deployment_name: openevolve-inference
deployment_tags:
  - openevolve
  - r1-distill-70b
  - 1xB200
